{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6dbeb90",
   "metadata": {},
   "source": [
    "# Machine Learning Frameworks Interoperability /<br> Zero-copy Recipes\n",
    "\n",
    "![zerocopy](https://developer-blogs.nvidia.com/wp-content/uploads/2021/08/ML_Chapter-1-1-1.png)\n",
    "\n",
    "This notebook and exercise is based on a 3-part blog series by Christian Hundt and Miguel Martinez from NVIDIA. You can revisit the material here:\n",
    "  - [Machine Learning Frameworks Interoperability, Part 1: Memory Layouts and Memory Pools](https://developer.nvidia.com/blog/machine-learning-frameworks-interoperability-part-1-memory-layouts-and-memory-pools/)\n",
    "  - [Machine Learning Frameworks Interoperability, Part 2: Data Loading and Data Transfer Bottlenecks](https://developer.nvidia.com/blog/machine-learning-frameworks-interoperability-part-2-data-loading-and-data-transfer-bottlenecks/)\n",
    "  - [Machine Learning Frameworks Interoperability, Part 3: Zero-Copy in Action using an E2E Pipeline](https://developer.nvidia.com/blog/machine-learning-frameworks-interoperability-part-3-zero-copy-in-action-using-an-e2e-pipeline/)\n",
    "  \n",
    "For a general overview over zero-copy operations, incl. small code examples for pair-wise array passing from framework to framework, please visit this [\"ML Frameworks Interoperability Cheat Sheet\"](http://bl.ocks.org/miguelusque/raw/f44a8e729896a96d0a3e4b07b5176af4/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e903319e",
   "metadata": {},
   "source": [
    "## Using Colab?\n",
    "\n",
    "Unfortunately, this notebook cannot be run in Google Colab yet, because it is not easily possible to pip-install missing libraries. It is best to run it on a local/remote compute resource where you can run docker containers. \n",
    "\n",
    "**Recommendation:** <br>Run this example e.g. in a docker container from the latest MONAI docker image on dockerhub (`docker pull projectmonai/monai`), which contains the necessary libraries to run this notebook (e.g. numba, cupy, RAPIDS cudf etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4537c",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data\n",
    "\n",
    "We will start loading a [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) file containing 20 hours of heartbeats.\n",
    "\n",
    "From the multiple libraries that can be used to read data from a CSV file ([pandas](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html), [NumPy](https://numpy.org/doc/stable/reference/generated/numpy.genfromtxt.html), â€¦), we have chosen to use [cuDF](https://docs.rapids.ai/api/cudf/stable/10min.html), a Python GPU-accelerated DataFrame library part of [RAPIDS](https://rapids.ai/) framework.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b627bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import urllib\n",
    "import zipfile\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def download_data(root='./data/ECG', csvfile='heartbeats.csv', url=None):\n",
    "        \n",
    "        assert url != None, \\\n",
    "        \"provide the URL to 22h of ECG data stated on the bottom of https://www.cs.ucr.edu/~eamonn/UCRsuite.html\"\n",
    "            \n",
    "        filename = os.path.join(root, 'ECG_one_day.zip')\n",
    "        csvfile = os.path.join(root, csvfile)\n",
    "        \n",
    "        if not os.path.isdir(root):\n",
    "            os.makedirs(root)\n",
    "        if not os.path.isfile(filename):\n",
    "            urllib.request.urlretrieve(url, filename)\n",
    "                    \n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(root)    \n",
    "    \n",
    "        data = loadmat(os.path.join(root, 'ECG_one_day','ECG.mat'))['ECG'].flatten()\n",
    "        \n",
    "        with open(csvfile, 'w', encoding='UTF8') as f:            \n",
    "            f.write('heartbeats\\n')\n",
    "            for date in data[:20000000]:\n",
    "                f.write('%s\\n' % date)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3cd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_data(url='https://www.cs.ucr.edu/~eamonn/ECG_one_day.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531693a",
   "metadata": {},
   "source": [
    "## Step 2: Parsing the CSV directly to GPU memory\n",
    "\n",
    "We use RAPIDS cudf -- a CUDA-accelerated pandas drop-in replacement -- to directly parse the CSV to fast GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "t0 = time.time()\n",
    "test = pd.read_csv(\"data/ECG/heartbeats.csv\", dtype='float32')\n",
    "print(f'Elapsed time: {time.time()-t0:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "t0 = time.time()\n",
    "heartbeats_cudf = cudf.read_csv(\"data/ECG/heartbeats.csv\", dtype='float32')\n",
    "print(f'Elapsed time: {time.time()-t0:.5f}')\n",
    "heartbeats_cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fdc11",
   "metadata": {},
   "source": [
    "Let's plot an excerpt of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8e7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "truncate = 1000\n",
    "pl.plot(heartbeats_cudf.to_pandas()[:truncate])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7f43e",
   "metadata": {},
   "source": [
    "## Step 3: QRS complex detection\n",
    "\n",
    "Let's sample a Ricker wavelet which is proportional to the (negative) second derivative of a Gaussian\n",
    "\n",
    "$g''(\\tau) \\propto - \\frac{d^2}{d\\tau^2} exp(-\\frac{\\tau^2}{2}) = exp(-\\frac{\\tau^2}{2}) (1-\\tau^2)$\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/0/08/MexicanHatMathematica.svg/2880px-MexicanHatMathematica.svg.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "Note that convolving a signal $f$ with the second derivative of a Gaussian $g$ is equivalent to smoothing $f$ with $g$ and then computing the second derivative, since\n",
    "\n",
    "$(f \\star g)' (t) = \\frac{d}{dt} \\int f(\\tau) g(t-\\tau) d \\tau = \\int f(\\tau) \\frac{d}{dt} g(t-\\tau) d \\tau = (f \\star g')(t)$\n",
    "\n",
    "and thus by induction:\n",
    "\n",
    "$(f \\star g)'' (t) = (f \\star g'')(t)$\n",
    "\n",
    "The standard deviation of the Gaussian governs the scale of the effect that we want to isolate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2437be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "\n",
    "def get_ricker(window, length):\n",
    "    \n",
    "    time = cp.linspace(-3, +3, window)\n",
    "    kernel = cp.zeros(length)\n",
    "    \n",
    "    # Using a ricker wavelet as bandpass filter (-) d^2/dt^2 exp(-0.5*t^2)    \n",
    "    # https://www.wolframalpha.com/input/?i=d%5E2%2Fdt%5E2+exp%28-0.5*t%5E2%29\n",
    "    # to isolate R in the QRS complex of each heartbeat\n",
    "    kernel[:window] = cp.exp(-0.5 * time**2) * (1 - time**2)\n",
    "    \n",
    "    return 2*kernel/cp.sqrt(window)\n",
    "\n",
    "wavelet = get_ricker(30, len(heartbeats_cudf))\n",
    "\n",
    "# Let's inspect the zero-embedded Ricker wavelet\n",
    "pl.plot(wavelet.get()[:100])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae489fa9",
   "metadata": {},
   "source": [
    "Subsequently, let us exploit the Fourier theorem to speed-up the convolution using the Fast Fourier Transform (FFT) in log-linear time.\n",
    "\n",
    "$\\mathcal F(f \\star g) = \\mathcal F(f) \\odot \\mathcal F(g) \\Rightarrow f \\star g = \\mathcal F^{-1} \\bigl(\\mathcal F(f) \\odot \\mathcal F(g)\\bigr)$\n",
    "\n",
    "The (Hermitian) FFT is implemented in CuPy and accelerated by cuFFT under the hood. Conversion from cudf to CuPy is accomplished via DLPack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fftconv(signal, kernel):\n",
    "    \n",
    "    # compute 'signal o kernel', where 'o' is 1D convolution, by exploiting Fourier theorem:\n",
    "    #    F(signal) * F(kernel) = F(signal o kernel)\n",
    "    # this can be done in O(n log n) time where n is the length of the signal\n",
    "\n",
    "    return cp.fft.irfft(cp.fft.rfft(signal)*cp.fft.rfft(kernel), n=len(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d7b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert heartbeats from cuDF to cupy\n",
    "heartbeats_cupy = cp.fromDlpack(heartbeats_cudf['heartbeats'].to_dlpack())\n",
    "    \n",
    "# compute smoothed curvature via ricker wavelet\n",
    "curvature_cupy = fftconv(heartbeats_cupy, wavelet)\n",
    "\n",
    "# plot both the signal as well as QRS response\n",
    "pl.plot(heartbeats_cudf.to_pandas()[:truncate])\n",
    "pl.plot(curvature_cupy.get()[:truncate])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bb16af",
   "metadata": {},
   "source": [
    "## Step 4: 1D Non-maximum suppression for robust local maxima detection\n",
    "\n",
    "Our goal is now to detect the local maxima in the curvature graph above to partition the stream into individual heartbeats. To achieve that we implement a basic CUDA kernel using the jit-compiler [Numba](http://numba.pydata.org/). The kernel determines for each value in the curvature signal if it is the maximum in a predefined window. In the following, we spawn a fixed number of CUDA threads being (re-)used to enumerate the whole range of positions in the stream. Note that the corresponding indexing scheme is called [grid-stride loop](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/). Each of the CUDA threads sequentially performs a max-reduction to determine the maximum. Although Numba comes with its own data types for device arrays, we can seamlessly pass CuPy arrays to Numba kernels without explicit type conversion. Both Numba and CuPy support the [CUDA array interface](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html) which allows for efficient zero-copy functionality across frameworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f88157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "\n",
    "@cuda.jit\n",
    "def segment_kernel(signal, window, out):\n",
    "    \n",
    "    base, step = cuda.grid(1), cuda.gridsize(1)    \n",
    "    for position in range(base, signal.shape[0] - window + 1, step):        \n",
    "        accum = -math.inf\n",
    "        for index in range(window):            \n",
    "            value = signal[position + index]\n",
    "            accum = value if value > accum else accum\n",
    "    \n",
    "        #                 am I the maximum in my neighborhood?\n",
    "        out[position] = 1 if accum == signal[position + (window + 1)//2] else 0\n",
    "\n",
    "def segment_numba(signal, window):\n",
    "    \n",
    "    out = cp.empty(signal.shape[0] - window + 1, dtype=cp.int64)\n",
    "    segment_kernel[80*32, 64](signal, window, out)    \n",
    "    cuda.synchronize()\n",
    "    \n",
    "    # fixes sequences of the following for 0000011000-> 0000010000\n",
    "    out[1:] *= (cp.diff(out) == 1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774013fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's segment the signal at the maximum peaks of the curvature graph\n",
    "gate_cupy = segment_numba(curvature_cupy, window=180)\n",
    "\n",
    "# let's plot the gates segmenting the stream\n",
    "pl.plot(heartbeats_cudf.to_pandas()[:truncate])\n",
    "pl.plot(gate_cupy.get()[:truncate])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b20ba0",
   "metadata": {},
   "source": [
    "## Step 5: Inspecting heart beat lengths\n",
    "\n",
    "The binary mask gate_cupy is 1 for each position that starts a heartbeat and 0 otherwise. Subsequently, we want to transform this dense representation with many zeroes to a sparse one where one only stores the indices in the stream that start a heartbeat. You could write a CUDA-kernel using [warp-aggregated atomics](https://developer.nvidia.com/blog/cuda-pro-tip-optimized-filtering-warp-aggregated-atomics/) for that purpose. In CuPy, however, this can be achieved easier by filtering the index domain with the predicate gate==1. An adjacent difference (discrete derivative cupy.diff) computes the heartbeat lengths as index distance between positive gate positions. Finally, the computed lengths are visualized in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e218f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indices_and_lengths_cupy(gate):\n",
    "\n",
    "    # all indices 0 1 2 3 4 5 6 ...\n",
    "    iota = cp.arange(len(gate))\n",
    "\n",
    "    # after filtering with gate==1 it becomes 3 6 10\n",
    "    indices = iota[gate == 1]\n",
    "    lengths = cp.diff(indices)\n",
    "    \n",
    "    return indices, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccce5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the segment lengths, we will later prune very long and short segments\n",
    "indices_cupy, lengths_cupy = indices_and_lengths_cupy(gate_cupy)\n",
    "\n",
    "# let's have a look at histogram of the heart beats lengths\n",
    "pl.hist(lengths_cupy.get(), bins=100)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d6d37",
   "metadata": {},
   "source": [
    "## Step 6: Candidate pruning and embedding in fixed length vectors\n",
    "\n",
    "In a later stage we intend to train a Variational Autoencoder (VAE) with fixed-length input and thus the heartbeats must be embedded in a data matrix of fixed shape. According to the histogram the majority of length is somewhere in the range between 100 and 250. The embedding is accomplished with Numba kernel. A warp of 32 consecutive threads works on each heartbeat. The first thread in a warp (leader) checks if the heartbeat exhibits a valid length and increments a row counter in an atomic manner to determine the output row in the data matrix. Subsequently, the target row is communicated to the remaining 31 threads in the warp using the warp-intrinsic shfl_sync (broadcast). In a final step, we (re-)use the threads in the warp to write the values to the output row in the data matrix in a warp-cyclic fashion (warp-stride loop). Finally, we plot a few of the zero-embedded heartbeats and observe approximate alignment of the QRS complex -- exactly what we wanted to achieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def zero_padding_kernel(signal, indices, counter, lower, upper, out):\n",
    "    \"\"\"using warp intrinsics to speedup the calcuation\"\"\"\n",
    "    \n",
    "    for candidate in range(cuda.blockIdx.x, indices.shape[0]-1, cuda.gridDim.x):\n",
    "        length = indices[candidate+1]-indices[candidate]\n",
    "        \n",
    "        # warp-centric: 32 threads process one signal\n",
    "        if lower <= length <= upper:\n",
    "            \n",
    "            entry = 0\n",
    "            if cuda.threadIdx.x == 0:\n",
    "                # here we select in thread 0 what will be the target row\n",
    "                entry = cuda.atomic.add(counter, 0, 1)\n",
    "            \n",
    "            # broadcast the target row to all other threads \n",
    "            # all 32 threads (warp) know the value\n",
    "            entry = cuda.shfl_sync(0xFFFFFFFF, entry, 0)  \n",
    "            \n",
    "            for index in range(cuda.threadIdx.x, upper, 32):                \n",
    "                out[entry, index] = signal[indices[candidate]+index] if index < length else 0.0\n",
    "                \n",
    "def zero_padding_numba(signal, indices, lengths, lower=100, upper=256):\n",
    "    \n",
    "    mask = (lower <= lengths) * (lengths <= upper)\n",
    "    num_entries = int(cp.sum(mask))\n",
    "    \n",
    "    out = cp.empty((num_entries, upper), dtype=signal.dtype)\n",
    "    counter = cp.zeros(1).astype(cp.int64)\n",
    "    zero_padding_kernel[80*32, 32](signal, indices, counter, lower, upper, out)    \n",
    "    cuda.synchronize()\n",
    "    \n",
    "    print(\"removed\", 100-100*num_entries/len(lengths), \"percent of the candidates\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's prune the short and long segments (heartbeats) and normalize them\n",
    "data_cupy = zero_padding_numba(heartbeats_cupy, indices_cupy, lengths_cupy, lower=100, upper=256)\n",
    "\n",
    "# Let's have a look at a few extracted heartbeats\n",
    "num_samples = 30\n",
    "for sample in data_cupy[:num_samples]:\n",
    "    pl.plot(sample.get())\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a2153",
   "metadata": {},
   "source": [
    "## Step 7: Training a Variational Autoencoder (VAE)\n",
    "\n",
    "In the last step, we train a Variational Autencoder (VAE) using Pytorch. We define the VAE network topology. Here, we use a convolutional version but you could also experiment with a classical [MLP VAE](https://github.com/pytorch/examples/blob/master/vae/main.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64088c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Swish(torch.nn.Module):    \n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()        \n",
    "        self.alpha = torch.nn.Parameter(torch.tensor([1.0], requires_grad=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x*torch.sigmoid(self.alpha.to(x.device)*x)\n",
    "\n",
    "class Downsample1d(torch.nn.Module):    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.filter = torch.tensor([1.0, 2.0, 1.0]).view(1, 1, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        w = torch.cat([self.filter]*x.shape[1], dim=0).to(x.device)\n",
    "        return torch.nn.functional.conv1d(x, w, stride=2, padding=1, groups=x.shape[1])\n",
    "\n",
    "class LightVAE(torch.nn.Module):\n",
    "    def __init__(self, num_dims):\n",
    "        super(LightVAE, self).__init__()\n",
    "        \n",
    "        self.num_dims = num_dims\n",
    "        assert num_dims & num_dims-1 == 0, \"num_dims must be power of 2\"\n",
    "        \n",
    "        self.down   = Downsample1d()\n",
    "        self.up     = torch.nn.Upsample(scale_factor=2)\n",
    "        self.sigma  = Swish()\n",
    "        \n",
    "        self.conv0  = torch.nn.Conv1d( 1,  4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv1  = torch.nn.Conv1d( 4,  8, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2  = torch.nn.Conv1d( 8, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.convA  = torch.nn.Conv1d(16,  2, kernel_size=3, stride=1, padding=1)\n",
    "        self.convB  = torch.nn.Conv1d(16,  2, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.restore = torch.nn.Linear(2, 16*num_dims//16)\n",
    "        \n",
    "        self.conv3  = torch.nn.Conv1d( 8, 4, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4  = torch.nn.Conv1d( 4, 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5  = torch.nn.Conv1d( 2, 1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        x  = x.view(-1, 1, self.num_dims)\n",
    "        x  = self.down(self.sigma(self.conv0(x)))\n",
    "        x  = self.down(self.sigma(self.conv1(x)))\n",
    "        x  = self.down(self.sigma(self.conv2(x)))\n",
    "        \n",
    "        return torch.mean(self.convA(x), dim=(2,)), \\\n",
    "               torch.mean(self.convB(x), dim=(2,))\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        \n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        \n",
    "        x = self.restore(z).view(-1, 8, self.num_dims//8)\n",
    "        x = self.sigma(self.conv3(self.up(x)))\n",
    "        x = self.sigma(self.conv4(self.up(x)))   \n",
    "              \n",
    "        return self.conv5(self.up(x)).view(-1, self.num_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "    \n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = torch.sum(torch.mean(torch.square(recon_x-x), dim=1))\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.1 * torch.sum(torch.mean(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))\n",
    "\n",
    "    return MSE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdb0076",
   "metadata": {},
   "source": [
    "Pytorch expects its dedicated tensor type and thus we need to map the CuPy array data_cupy to a FloatTensor. We perform that again using zero-copy functionality via DLPack. The remaining code is plain Pytorch program that trains the VAE on the training set for 10 epochs using the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-copy to pytorch tensors using dlpack\n",
    "from torch.utils import dlpack\n",
    "\n",
    "cp.random.seed(42)\n",
    "cp.random.shuffle(data_cupy)\n",
    "\n",
    "split = int(0.75*len(data_cupy))\n",
    "trn_torch = dlpack.from_dlpack(data_cupy[:split].toDlpack())\n",
    "tst_torch = dlpack.from_dlpack(data_cupy[split:].toDlpack())\n",
    "\n",
    "dim = trn_torch.shape[1]\n",
    "model = LightVAE(dim).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667f1e86",
   "metadata": {},
   "source": [
    "Let's train the VAE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea6d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "trn_loader = torch.utils.data.DataLoader(trn_torch, batch_size=BATCH_SIZE, shuffle=True)\n",
    "tst_loader = torch.utils.data.DataLoader(tst_torch, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trn_loss = 0.0\n",
    "    for data in trn_loader:\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        trn_loss += loss.item()\n",
    "        optimizer.step()\n",
    "       \n",
    "    print('====> Epoch: {} Average loss: {:.7f}'.format(\n",
    "          epoch, trn_loss / len(trn_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bb65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it will be used to visualize a scatter char\n",
    "mu_cudf = cudf.DataFrame()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    tst_loss = 0\n",
    "    for data in tst_loader:\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        tst_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "        mu_cudf = mu_cudf.append(cudf.DataFrame(mu, columns=['x', 'y']))\n",
    "            \n",
    "tst_loss /= len(tst_loader.dataset)\n",
    "print('====> Test set loss: {:.4f}'.format(tst_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56a3382",
   "metadata": {},
   "source": [
    "## Step 8: Visualizing the latent space and sampling heartbeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8309c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = mu_cudf.to_pandas()\n",
    "pl.plot(embedding['x'], embedding['y'], 'x')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973961a9",
   "metadata": {},
   "source": [
    " Sampling heartbeats from a Gaussian in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e05fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "sampling_range = 5\n",
    "with torch.no_grad():\n",
    "    x, y = [torch.linspace(-sampling_range, sampling_range, 5)]*2\n",
    "    grid_x, grid_y = torch.meshgrid(x, y, indexing='ij')\n",
    "    latents = torch.column_stack((torch.flatten(grid_x), \n",
    "                                  torch.flatten(grid_y))).to('cuda')\n",
    "    samples = model.decode(latents).cpu()\n",
    "\n",
    "for index, (sample, latent) in enumerate(zip(samples, latents)):\n",
    "    pl.subplot(5, 5, index+1)\n",
    "    pl.subplots_adjust(hspace=0.6)\n",
    "    pl.title(str(tuple(latent.cpu().numpy().round(1))))\n",
    "    pl.xticks([])\n",
    "    pl.yticks([])\n",
    "    pl.plot(sample, linewidth=0.5)\n",
    "\n",
    "pl.savefig(\"fake_heartbeats.pdf\", dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0ca484",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "Try reimplementing the VAE with MONAI's [VarAutoEncoder](https://docs.monai.io/en/stable/networks.html#varautoencoder) class. \n",
    "\n",
    "A few hints:\n",
    "* Like most other MONAI network architectures, `VarAutoEncoder` can be used for 1D/2D/3D data. Make sure to set `spatial_dims=1` in the constructor to auto-encode 1D sequences.\n",
    "* The MONAI-tutorial repository features a [full tutorial](https://github.com/Project-MONAI/tutorials/blob/main/modules/varautoencoder_mednist.ipynb) on how to use in a 2D image auto-encoding scenario is given for the MedNIST dataset. Check it out to learn about VarAutoEncoder's detailed usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b982abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill here, and rerun training\n",
    "\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
